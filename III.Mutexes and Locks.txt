Mutexes and Locks

3.2 Using a Mutex To Protect Shared Data
    The mutex entity
    Until now, the methods we have used to pass data between threads were short-term and involved passing an argument (the promise) 
    from a parent thread to a worker thread and then passing a result back to the parent thread (via the future) once it has become available. 
    The promise-future construct is a non-permanent communication channel for one-time usage.
    We have seen that in order to avoid data races, we need to either forego accessing shared data or use it in read-only access 
    without mutating the data. In this chapter, we want to look at a way to establish a stable long-term communication channel 
    that allows for both sharing and mutation. Ideally, we would like to have a communication protocol that corresponds to 
    voice communication over a radio channel, where the transmitter uses the expression "over" to indicate the end of the transmission 
    to the receiver. By using such a protocol, sender and receiver can take turns in transmitting their data. 
    In C++, this concept of taking turns can be constructed by an entity called a "mutex" - which stands for MUtual EXclusion.

    Recall that a data race requires simultaneous access from two threads. 
    If we can guarantee that only a single thread at a time can access a particular memory location, data races would not occur. 
    In order for this to work, we would need to establish a communication protocol. 
    It is important to note that a mutex is not the solution to the data race problem per se but merely an enabler 
    for a thread-safe communication protocol that has to be implemented and adhered to by the programmer.

    Let us take a look at how this protocol works: 
    Assuming we have a piece of memory (e.g. a shared variable) that we want to protect from simultaneous access, 
    we can assign a mutex to be the guardian of this particular memory. It is important to understand that 
    a mutex is bound to the memory it protects. A thread 1 who wants to access the protected memory must "lock" the mutex first. 
    After thread 1 is "under the lock", a thread 2 is blocked from access to the shared variable, it can not acquire the lock on the mutex and
    is temporarily suspended by the system.

    Once the reading or writing operation of thread 1 is complete, it must "unlock" the mutex so that thread 2 can access the memory location.
    Often, the code which is executed "under the lock" is referred to as a "critical section". 
    It is important to note that also read-only access to the shared memory has to lock the mutex to prevent a data race 
    - which would happen when another thread, who might be under the lock at that time, were to modify the data.

    When several threads were to try to acquire and lock the mutex, only one of them would be successful. 
    All other threads would automatically be put on hold - just as cars waiting at an intersection for a green light.
    Once the thread who has succeeded in acquiring the lock had finished its job and unlocked the mutex, 
    a queued thread waiting for access would be woken up and allowed to lock the mutex to proceed with his read / write operation. 
    If all threads were to follow this protocol, a data race would effectively be avoided. Before we take a closer look at such a protocol, 
    let us analyze a code example3.2.1.cpp.

    This code builds on some of the classes we have seen in the previous lesson project - the concurrent traffic simulation. 
    There is a class Vehicle that has a single data member (int _id). 
    Also, there is a class WaitingVehicles, which is supposed to store a number of vehicles in an internal vector. 
    Note that contrary to the lesson project, a vehicle is moved into the vector using an rvalue reference. 
    Also note that the push_back function is commented out here. The reason for this is that we are trying to provoke a data race 
    - leaving push_back active would cause the program to crash (we will comment it in later). 
    This is also the reason why there is an auxiliary member _tmpVehicles, which will be used to count the number of Vehicles added 
    via calls to pushBack(). This temporary variable will help us expose the data race without crashing the program.

    In main(), a for-loop is used to launch a large number of tasks who all try to add a newly created Vehicle to the queue. 
    Running the program synchronously with launch option std::launch::deferred generates the following output on the console:
    #Vehicles = 1000

    This code builds on some of the classes we have seen in the previous lesson project - the concurrent traffic simulation. 
    There is a class Vehicle that has a single data member (int _id). 
    Also, there is a class WaitingVehicles, which is supposed to store a number of vehicles in an internal vector. 
    Note that contrary to the lesson project, a vehicle is moved into the vector using an rvalue reference. 
    Also note that the push_back function is commented out here. The reason for this is that we are trying to provoke a data race 
    - leaving push_back active would cause the program to crash (we will comment it in later). 
    This is also the reason why there is an auxiliary member _tmpVehicles, which will be used to count the number of Vehicles added 
    via calls to pushBack(). This temporary variable will help us expose the data race without crashing the program.

    In main(), a for-loop is used to launch a large number of tasks who all try to add a newly created Vehicle to the queue. 
    Running the program synchronously with launch option std::launch::deferred generates the following output on the console:
    #Vehicles = 992

    It seems that not all the vehicles could be added to the queue. But why is that? 
    Note that in the thread function "pushBack" there is a call to sleep_for, which pauses the thread execution for a short time. 
    This is the position where the data race occurs: First, the current value of _tmpVehicles is stored in a temporary variable oldNum. 
    While the thread is paused, there might (and will) be changes to _tmpVehicles performed by other threads. 
    When the execution resumes, the former value of _tmpVehicles is written back, thus invalidating the contribution of all the threads 
    who had write access in the mean time. 
    Interestingly, when sleep_for is commented out, the output of the program is the same as with std::launch::deferred 
    - at least that will be the case for most of the time when we run the program. 
    But once in a while, there might be a scheduling constellation which causes the bug to expose itself. 
    Apart from understanding the data race, you should take as an advice that introducing deliberate time delays 
    in the testing / debugging phase of development can help expose many concurrency bugs.



    Using mutex to protect data
    In its simplest form, using a mutex consists of four straight-forward steps:
    1.Include the <mutex> header
    2.Create an std::mutex
    3.Lock the mutex using lock() before read/write is called
    4.Unlock the mutex after the read/write operation is finished using unlock()
    
    In order to protect the access to _vehicles from being manipulated by several threads at once, 
    a mutex has been added to the class as a private data member. 
    In the pushBack function, the mutex is locked before a new element is added to the vector and unlocked after the operation is complete.

    Note that the mutex is also locked in the function printSize just before printing the size of the vector. 
    The reason for this lock is two-fold: 
    First, we want to prevent a data race that would occur when a read-access to the vector and a simultaneous write access 
    (even when under the lock) would occur. 
    And second, we want to exclusively reserve the standard output to the console for printing the vector size 
    without other threads printing to it at the same time.

    When this code is executed, 1000 elements will be in the vector. By using a mutex to our shared resource, 
    a data race has been effectively avoided.



    Using timed_mutex
    In the following, a short overview of the different available mutex types is given:
    1.mutex: provides the core functions lock() and unlock() and the non-blocking try_lock() method that returns 
    if the mutex is not available.
    2.recursive_mutex: allows multiple acquisitions of the mutex from the same thread.
    3.timed_mutex: similar to mutex, but it comes with two more methods try_lock_for() and try_lock_until() 
    that try to acquire the mutex for a period of time or until a moment in time is reached.
    4.recursive_timed_mutex: is a combination of timed_mutex and recursive_mutex.

    Deadlock 1
    Using mutexes can significantly reduce the risk of data races as seen in the example above. 
    But imagine what would happen if an exception was thrown while executing code in the critical section, 
    i.e. between lock and unlock. In such a case, the mutex would remain locked indefinitely and no other thread could unlock it 
    - the program would most likely freeze. 

    Deadlock 2
    A second type of deadlock is a state in which two or more threads are blocked because each thread waits 
    for the resource of the other thread to be released before releasing its resource. 
    The result of the deadlock is a complete standstill. The thread and therefore usually the whole program is blocked forever.
